{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e774aa14-fc37-451c-877f-7e4e36592088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 10:20:56,712 - INFO - Running scalability analysis for dataset_size=small, cluster_size=2\n",
      "2025-03-24 10:20:56,715 - INFO - Starting processing for dataset_size=small, cluster_size=2\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/24 10:20:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-24 10:21:01,773 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:01,817 - INFO - Time elapsed: 0.04 seconds\n",
      "2025-03-24 10:21:01,819 - INFO - Memory usage: 9.80 MB\n",
      "2025-03-24 10:21:01,821 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:01,824 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:03,046 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:03,049 - INFO - Completed processing: time=6.33s, CPU=4.15%, memory=12.21MB\n",
      "2025-03-24 10:21:08,062 - INFO - Running scalability analysis for dataset_size=small, cluster_size=4\n",
      "2025-03-24 10:21:08,065 - INFO - Starting processing for dataset_size=small, cluster_size=4\n",
      "2025-03-24 10:21:08,341 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:08,345 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:08,345 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:08,347 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:08,349 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:09,831 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:09,834 - INFO - Completed processing: time=1.77s, CPU=5.20%, memory=0.16MB\n",
      "2025-03-24 10:21:14,846 - INFO - Running scalability analysis for dataset_size=small, cluster_size=8\n",
      "2025-03-24 10:21:14,849 - INFO - Starting processing for dataset_size=small, cluster_size=8\n",
      "2025-03-24 10:21:15,074 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:15,076 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:15,077 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:15,078 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:15,079 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:16,559 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:16,562 - INFO - Completed processing: time=1.71s, CPU=5.90%, memory=0.00MB\n",
      "2025-03-24 10:21:21,572 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=2\n",
      "2025-03-24 10:21:21,575 - INFO - Starting processing for dataset_size=medium, cluster_size=2\n",
      "2025-03-24 10:21:21,811 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:21,815 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:21,816 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:21,818 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:21:21,819 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:22,867 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:21:22,874 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:22,876 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:22,878 - INFO - CPU usage: 25.00%\n",
      "2025-03-24 10:21:22,880 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:21:24,293 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:24,297 - INFO - Completed processing: time=2.72s, CPU=3.70%, memory=0.00MB\n",
      "2025-03-24 10:21:29,307 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=4\n",
      "2025-03-24 10:21:29,309 - INFO - Starting processing for dataset_size=medium, cluster_size=4\n",
      "2025-03-24 10:21:29,490 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:29,493 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:29,494 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:29,495 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:21:29,496 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:30,540 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:21:30,546 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:30,549 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:30,551 - INFO - CPU usage: 25.00%\n",
      "2025-03-24 10:21:30,552 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:21:31,974 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:31,978 - INFO - Completed processing: time=2.66s, CPU=4.37%, memory=0.00MB\n",
      "2025-03-24 10:21:36,987 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=8\n",
      "2025-03-24 10:21:36,990 - INFO - Starting processing for dataset_size=medium, cluster_size=8\n",
      "2025-03-24 10:21:37,164 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:37,166 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:37,167 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:37,168 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:21:37,169 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:38,214 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:21:38,220 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:38,223 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:38,225 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:38,226 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:21:39,650 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:39,654 - INFO - Completed processing: time=2.66s, CPU=8.03%, memory=0.00MB\n",
      "2025-03-24 10:21:44,667 - INFO - Running scalability analysis for dataset_size=large, cluster_size=2\n",
      "2025-03-24 10:21:44,670 - INFO - Starting processing for dataset_size=large, cluster_size=2\n",
      "2025-03-24 10:21:44,862 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:44,865 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:44,866 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:44,867 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:44,868 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:45,932 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:21:45,938 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:45,940 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:45,941 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:45,943 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:21:47,007 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-24 10:21:47,013 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:47,015 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:47,017 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:21:47,019 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc'\n",
      "2025-03-24 10:21:48,349 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:48,352 - INFO - Completed processing: time=3.68s, CPU=5.60%, memory=0.00MB\n",
      "2025-03-24 10:21:53,361 - INFO - Running scalability analysis for dataset_size=large, cluster_size=4\n",
      "2025-03-24 10:21:53,364 - INFO - Starting processing for dataset_size=large, cluster_size=4\n",
      "2025-03-24 10:21:53,535 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:21:53,537 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:53,538 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:53,539 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:21:53,540 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:21:54,594 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:21:54,600 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:54,602 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:54,604 - INFO - CPU usage: 25.00%\n",
      "2025-03-24 10:21:54,606 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:21:55,668 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-24 10:21:55,674 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:21:55,676 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:21:55,678 - INFO - CPU usage: 25.00%\n",
      "2025-03-24 10:21:55,683 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc'\n",
      "2025-03-24 10:21:57,015 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:21:57,018 - INFO - Completed processing: time=3.65s, CPU=6.72%, memory=0.00MB\n",
      "2025-03-24 10:22:02,029 - INFO - Running scalability analysis for dataset_size=large, cluster_size=8\n",
      "2025-03-24 10:22:02,032 - INFO - Starting processing for dataset_size=large, cluster_size=8\n",
      "2025-03-24 10:22:02,207 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-24 10:22:02,209 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:22:02,210 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:22:02,211 - INFO - CPU usage: 50.00%\n",
      "2025-03-24 10:22:02,212 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc'\n",
      "2025-03-24 10:22:03,264 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-24 10:22:03,271 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:22:03,273 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:22:03,275 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:22:03,276 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc'\n",
      "2025-03-24 10:22:04,327 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-24 10:22:04,334 - INFO - Time elapsed: 0.00 seconds\n",
      "2025-03-24 10:22:04,336 - INFO - Memory usage: 0.00 MB\n",
      "2025-03-24 10:22:04,338 - INFO - CPU usage: 0.00%\n",
      "2025-03-24 10:22:04,340 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [Errno 2] No such file or directory: '/home/ubuntu/DataEngineering/project/results/scalability_results/tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc'\n",
      "2025-03-24 10:22:05,693 - INFO - Spark session stopped successfully\n",
      "2025-03-24 10:22:05,697 - INFO - Completed processing: time=3.66s, CPU=8.12%, memory=0.00MB\n",
      "2025-03-24 10:22:10,721 - INFO - Scalability analysis complete. Results saved to scalability_results.csv\n",
      "2025-03-24 10:22:10,722 - INFO - Summary saved to scalability_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, avg\n",
    "import xarray as xr\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"scalability_analysis.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Configuration for scalability analysis\n",
    "DATASET_SIZES = [\"small\", \"medium\", \"large\"]  # Define dataset sizes\n",
    "CLUSTER_SIZES = [2, 4, 8]  # Define cluster sizes \n",
    "OUTPUT_DIR = \"scalability_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "@contextmanager\n",
    "def memory_monitoring():\n",
    "    \"\"\"Context manager to monitor memory usage during operations.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    cpu_before = psutil.cpu_percent()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        mem_after = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        \n",
    "        logger.info(f\"Time elapsed: {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Memory usage: {mem_after - mem_before:.2f} MB\")\n",
    "        logger.info(f\"CPU usage: {(cpu_before + cpu_after) / 2:.2f}%\")\n",
    "\n",
    "def get_spark_session(cluster_size):\n",
    "    \"\"\"Create and configure a Spark session with appropriate resources.\"\"\"\n",
    "    try:\n",
    "        # Configure Spark with memory constraints and cluster size\n",
    "        return SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"scalability_group_28\")\\\n",
    "            .config(\"spark.executor.memory\", \"2g\")\\\n",
    "            .config(\"spark.driver.memory\", \"4g\")\\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"true\")\\\n",
    "            .config(\"spark.memory.offHeap.size\", \"2g\")\\\n",
    "            .config(\"spark.executor.cores\", str(cluster_size))\\\n",
    "            .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_netcdf_chunk(file_path, spark, chunk_size=500):\n",
    "    \"\"\"Process a NetCDF file in chunks to avoid memory issues.\"\"\"\n",
    "    logger.info(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with memory_monitoring():\n",
    "            # Open the NetCDF file with chunking options\n",
    "            with xr.open_dataset(file_path, chunks={'time': chunk_size}) as ds:\n",
    "                # Instead of loading the entire dataset at once, we'll process it in chunks\n",
    "                yearly_avgs = []\n",
    "                \n",
    "                # Get unique years\n",
    "                time_values = ds.time.values\n",
    "                years = np.unique(pd.DatetimeIndex(time_values).year)\n",
    "                \n",
    "                # Process each year separately\n",
    "                for yr in years:\n",
    "                    logger.info(f\"Processing year {yr}\")\n",
    "                    # Filter dataset for current year\n",
    "                    year_ds = ds.sel(time=ds.time.dt.year == yr)\n",
    "                    \n",
    "                    # Extract necessary variables \n",
    "                    if 'tg' in year_ds:\n",
    "                        # Calculate the mean for this year\n",
    "                        yearly_avg = year_ds.tg.mean().compute()\n",
    "                        yearly_avgs.append((yr, float(yearly_avg)))\n",
    "                    \n",
    "                    # garbage collection\n",
    "                    del year_ds\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Create spark dataframe \n",
    "                if yearly_avgs:\n",
    "                    schema = [\"year\", \"yearly_avg_temperature\"]\n",
    "                    spark_df = spark.createDataFrame(yearly_avgs, schema=schema)\n",
    "                    # Cache small result\n",
    "                    spark_df.cache()\n",
    "                    # Collect results \n",
    "                    results = spark_df.collect()\n",
    "                    spark_df.unpersist()\n",
    "                    return results\n",
    "                return []\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def run_netcdf_processing(dataset_size, cluster_size):\n",
    "    \"\"\"\n",
    "    Run the NetCDF processing script with the given dataset size and cluster size.\n",
    "    \"\"\"\n",
    "    # Define file paths based on dataset size\n",
    "    if dataset_size == \"small\":\n",
    "        file_paths = [\"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\"]\n",
    "    elif dataset_size == \"medium\":\n",
    "        file_paths = [\n",
    "            \"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\"\n",
    "        ]\n",
    "    elif dataset_size == \"large\":\n",
    "        file_paths = [\n",
    "            \"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\"\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset size\")\n",
    "    \n",
    "    logger.info(f\"Starting processing for dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "    \n",
    "    # Initialize metrics\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_usage_samples = []\n",
    "    memory_usage_start = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "    spark = None\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Start Spark session with the given cluster size\n",
    "        spark = get_spark_session(cluster_size)\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            # Sample CPU usage periodically\n",
    "            cpu_usage_samples.append(psutil.cpu_percent())\n",
    "            \n",
    "            # Process the file\n",
    "            file_results = process_netcdf_chunk(file_path, spark)\n",
    "            results.extend(file_results)\n",
    "            \n",
    "            # Garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Let system resources recover between files\n",
    "            time.sleep(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in processing run: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Stop the Spark session\n",
    "        if spark:\n",
    "            try:\n",
    "                spark.stop()\n",
    "                logger.info(\"Spark session stopped successfully\")\n",
    "            except:\n",
    "                logger.warning(\"Error stopping Spark session\")\n",
    "    \n",
    "    # Log end time and resource usage\n",
    "    end_time = time.time()\n",
    "    cpu_usage_samples.append(psutil.cpu_percent())\n",
    "    memory_usage_end = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Calculate metrics\n",
    "    processing_time = end_time - start_time\n",
    "    cpu_usage = sum(cpu_usage_samples) / len(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "    memory_usage = memory_usage_end - memory_usage_start\n",
    "    \n",
    "    logger.info(f\"Completed processing: time={processing_time:.2f}s, CPU={cpu_usage:.2f}%, memory={memory_usage:.2f}MB\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"cluster_size\": cluster_size,\n",
    "        \"processing_time\": processing_time,\n",
    "        \"cpu_usage\": cpu_usage,\n",
    "        \"memory_usage\": memory_usage,\n",
    "        \"result_count\": len(results)\n",
    "    }\n",
    "\n",
    "def perform_scalability_analysis():\n",
    "    \"\"\"\n",
    "    Perform scalability analysis by running the NetCDF processing script with different configurations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create a checkpoint file \n",
    "    checkpoint_file = os.path.join(OUTPUT_DIR, \"checkpoint.csv\")\n",
    "    completed_configs = set()\n",
    "    \n",
    "    # Check if checkpoint exists and load completed configurations\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint_df = pd.read_csv(checkpoint_file)\n",
    "        for _, row in checkpoint_df.iterrows():\n",
    "            completed_configs.add((row['dataset_size'], row['cluster_size']))\n",
    "        logger.info(f\"Loaded {len(completed_configs)} completed configurations from checkpoint\")\n",
    "        results = checkpoint_df.to_dict('records')\n",
    "    \n",
    "    # Run the analysis for each configuration\n",
    "    for dataset_size in DATASET_SIZES:\n",
    "        for cluster_size in CLUSTER_SIZES:\n",
    "            # Skip configurations that have already been completed\n",
    "            if (dataset_size, cluster_size) in completed_configs:\n",
    "                logger.info(f\"Skipping completed configuration: dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Running scalability analysis for dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "            \n",
    "            try:\n",
    "                # Run the processing with this configuration\n",
    "                result = run_netcdf_processing(dataset_size, cluster_size)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update checkpoint file after each successful configuration\n",
    "                pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "                completed_configs.add((dataset_size, cluster_size))\n",
    "                \n",
    "                # Stabilize the system \n",
    "                time.sleep(5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed analysis for dataset_size={dataset_size}, cluster_size={cluster_size}: {e}\")\n",
    "    \n",
    "    # Save final results to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(OUTPUT_DIR, \"scalability_results.csv\"), index=False)\n",
    "    \n",
    "    # Generate a simple summary\n",
    "    summary = results_df.groupby(['dataset_size', 'cluster_size']).agg({\n",
    "        'processing_time': 'mean',\n",
    "        'cpu_usage': 'mean',\n",
    "        'memory_usage': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    summary.to_csv(os.path.join(OUTPUT_DIR, \"scalability_summary.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Scalability analysis complete. Results saved to scalability_results.csv\")\n",
    "    logger.info(\"Summary saved to scalability_summary.csv\")\n",
    "\n",
    "# Run scalability analysis\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        perform_scalability_analysis()\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unhandled exception in main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34b92f-d685-479b-9fe7-589a48743ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
