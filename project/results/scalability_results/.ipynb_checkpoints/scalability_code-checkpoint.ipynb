{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e774aa14-fc37-451c-877f-7e4e36592088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 09:51:54,322 - INFO - Running scalability analysis for dataset_size=small, cluster_size=2\n",
      "2025-03-23 09:51:54,326 - INFO - Starting processing for dataset_size=small, cluster_size=2\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/23 09:52:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2025-03-23 09:52:25,377 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:52:36,588 - INFO - Processing year 1980\n",
      "2025-03-23 09:52:38,073 - INFO - Processing year 1981\n",
      "2025-03-23 09:52:39,922 - INFO - Processing year 1982\n",
      "2025-03-23 09:52:42,017 - INFO - Processing year 1983\n",
      "2025-03-23 09:52:43,070 - INFO - Processing year 1984\n",
      "2025-03-23 09:52:44,909 - INFO - Processing year 1985\n",
      "2025-03-23 09:52:47,633 - INFO - Processing year 1986\n",
      "2025-03-23 09:52:49,526 - INFO - Processing year 1987\n",
      "2025-03-23 09:52:50,526 - INFO - Processing year 1988\n",
      "2025-03-23 09:52:52,448 - INFO - Processing year 1989\n",
      "2025-03-23 09:52:54,639 - INFO - Processing year 1990\n",
      "2025-03-23 09:52:56,755 - INFO - Processing year 1991\n",
      "2025-03-23 09:52:57,780 - INFO - Processing year 1992\n",
      "2025-03-23 09:52:59,851 - INFO - Processing year 1993\n",
      "2025-03-23 09:53:01,747 - INFO - Processing year 1994\n",
      "2025-03-23 09:53:08,209 - INFO - Time elapsed: 42.83 seconds\n",
      "2025-03-23 09:53:08,212 - INFO - Memory usage: 91.73 MB\n",
      "2025-03-23 09:53:08,216 - INFO - CPU usage: 10.75%\n",
      "2025-03-23 09:53:08,217 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:53:09,874 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:53:09,878 - INFO - Completed processing: time=75.55s, CPU=6.25%, memory=93.77MB\n",
      "2025-03-23 09:53:15,117 - INFO - Running scalability analysis for dataset_size=small, cluster_size=4\n",
      "2025-03-23 09:53:15,119 - INFO - Starting processing for dataset_size=small, cluster_size=4\n",
      "2025-03-23 09:53:15,355 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:53:15,377 - INFO - Processing year 1980\n",
      "2025-03-23 09:53:16,393 - INFO - Processing year 1981\n",
      "2025-03-23 09:53:17,820 - INFO - Processing year 1982\n",
      "2025-03-23 09:53:19,332 - INFO - Processing year 1983\n",
      "2025-03-23 09:53:20,409 - INFO - Processing year 1984\n",
      "2025-03-23 09:53:21,864 - INFO - Processing year 1985\n",
      "2025-03-23 09:53:23,303 - INFO - Processing year 1986\n",
      "2025-03-23 09:53:24,763 - INFO - Processing year 1987\n",
      "2025-03-23 09:53:25,782 - INFO - Processing year 1988\n",
      "2025-03-23 09:53:27,196 - INFO - Processing year 1989\n",
      "2025-03-23 09:53:28,901 - INFO - Processing year 1990\n",
      "2025-03-23 09:53:30,377 - INFO - Processing year 1991\n",
      "2025-03-23 09:53:31,366 - INFO - Processing year 1992\n",
      "2025-03-23 09:53:32,805 - INFO - Processing year 1993\n",
      "2025-03-23 09:53:34,216 - INFO - Processing year 1994\n",
      "2025-03-23 09:53:35,223 - INFO - Time elapsed: 19.87 seconds\n",
      "2025-03-23 09:53:35,225 - INFO - Memory usage: 35.20 MB\n",
      "2025-03-23 09:53:35,226 - INFO - CPU usage: 16.90%\n",
      "2025-03-23 09:53:35,228 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:53:36,447 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:53:36,451 - INFO - Completed processing: time=21.33s, CPU=4.70%, memory=35.32MB\n",
      "2025-03-23 09:53:41,464 - INFO - Running scalability analysis for dataset_size=small, cluster_size=8\n",
      "2025-03-23 09:53:41,467 - INFO - Starting processing for dataset_size=small, cluster_size=8\n",
      "2025-03-23 09:53:41,689 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:53:41,715 - INFO - Processing year 1980\n",
      "2025-03-23 09:53:42,734 - INFO - Processing year 1981\n",
      "2025-03-23 09:53:44,330 - INFO - Processing year 1982\n",
      "2025-03-23 09:53:45,743 - INFO - Processing year 1983\n",
      "2025-03-23 09:53:46,731 - INFO - Processing year 1984\n",
      "2025-03-23 09:53:48,296 - INFO - Processing year 1985\n",
      "2025-03-23 09:53:49,992 - INFO - Processing year 1986\n",
      "2025-03-23 09:53:51,711 - INFO - Processing year 1987\n",
      "2025-03-23 09:53:52,714 - INFO - Processing year 1988\n",
      "2025-03-23 09:53:54,221 - INFO - Processing year 1989\n",
      "2025-03-23 09:53:55,739 - INFO - Processing year 1990\n",
      "2025-03-23 09:53:57,196 - INFO - Processing year 1991\n",
      "2025-03-23 09:53:58,185 - INFO - Processing year 1992\n",
      "2025-03-23 09:53:59,604 - INFO - Processing year 1993\n",
      "2025-03-23 09:54:01,028 - INFO - Processing year 1994\n",
      "2025-03-23 09:54:02,208 - INFO - Time elapsed: 20.52 seconds\n",
      "2025-03-23 09:54:02,210 - INFO - Memory usage: 22.19 MB\n",
      "2025-03-23 09:54:02,211 - INFO - CPU usage: 41.05%\n",
      "2025-03-23 09:54:02,213 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:54:03,811 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:54:03,815 - INFO - Completed processing: time=22.35s, CPU=3.65%, memory=22.19MB\n",
      "2025-03-23 09:54:08,827 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=2\n",
      "2025-03-23 09:54:08,830 - INFO - Starting processing for dataset_size=medium, cluster_size=2\n",
      "2025-03-23 09:54:09,080 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:54:09,101 - INFO - Processing year 1980\n",
      "2025-03-23 09:54:10,354 - INFO - Processing year 1981\n",
      "2025-03-23 09:54:11,771 - INFO - Processing year 1982\n",
      "2025-03-23 09:54:13,260 - INFO - Processing year 1983\n",
      "2025-03-23 09:54:14,407 - INFO - Processing year 1984\n",
      "2025-03-23 09:54:15,880 - INFO - Processing year 1985\n",
      "2025-03-23 09:54:17,288 - INFO - Processing year 1986\n",
      "2025-03-23 09:54:18,995 - INFO - Processing year 1987\n",
      "2025-03-23 09:54:19,991 - INFO - Processing year 1988\n",
      "2025-03-23 09:54:21,634 - INFO - Processing year 1989\n",
      "2025-03-23 09:54:23,259 - INFO - Processing year 1990\n",
      "2025-03-23 09:54:24,970 - INFO - Processing year 1991\n",
      "2025-03-23 09:54:26,215 - INFO - Processing year 1992\n",
      "2025-03-23 09:54:27,908 - INFO - Processing year 1993\n",
      "2025-03-23 09:54:29,583 - INFO - Processing year 1994\n",
      "2025-03-23 09:54:30,566 - INFO - Time elapsed: 21.48 seconds\n",
      "2025-03-23 09:54:30,568 - INFO - Memory usage: 0.02 MB\n",
      "2025-03-23 09:54:30,569 - INFO - CPU usage: 16.75%\n",
      "2025-03-23 09:54:30,570 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:54:31,631 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:54:31,728 - INFO - Processing year 1995\n",
      "2025-03-23 09:54:33,246 - INFO - Processing year 1996\n",
      "2025-03-23 09:54:35,046 - INFO - Processing year 1997\n",
      "2025-03-23 09:54:36,809 - INFO - Processing year 1998\n",
      "2025-03-23 09:54:37,814 - INFO - Processing year 1999\n",
      "2025-03-23 09:54:40,028 - INFO - Processing year 2000\n",
      "2025-03-23 09:54:41,994 - INFO - Processing year 2001\n",
      "2025-03-23 09:54:44,100 - INFO - Processing year 2002\n",
      "2025-03-23 09:54:45,199 - INFO - Processing year 2003\n",
      "2025-03-23 09:54:47,404 - INFO - Processing year 2004\n",
      "2025-03-23 09:54:49,532 - INFO - Processing year 2005\n",
      "2025-03-23 09:54:51,490 - INFO - Processing year 2006\n",
      "2025-03-23 09:54:52,701 - INFO - Processing year 2007\n",
      "2025-03-23 09:54:54,779 - INFO - Processing year 2008\n",
      "2025-03-23 09:54:56,701 - INFO - Processing year 2009\n",
      "2025-03-23 09:54:57,746 - INFO - Processing year 2010\n",
      "2025-03-23 09:54:59,158 - INFO - Time elapsed: 27.52 seconds\n",
      "2025-03-23 09:54:59,161 - INFO - Memory usage: 0.12 MB\n",
      "2025-03-23 09:54:59,165 - INFO - CPU usage: 39.10%\n",
      "2025-03-23 09:54:59,168 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:55:00,292 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:55:00,295 - INFO - Completed processing: time=51.46s, CPU=4.47%, memory=0.14MB\n",
      "2025-03-23 09:55:05,305 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=4\n",
      "2025-03-23 09:55:05,308 - INFO - Starting processing for dataset_size=medium, cluster_size=4\n",
      "2025-03-23 09:55:05,528 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:55:05,548 - INFO - Processing year 1980\n",
      "2025-03-23 09:55:06,556 - INFO - Processing year 1981\n",
      "2025-03-23 09:55:07,997 - INFO - Processing year 1982\n",
      "2025-03-23 09:55:09,454 - INFO - Processing year 1983\n",
      "2025-03-23 09:55:10,473 - INFO - Processing year 1984\n",
      "2025-03-23 09:55:12,056 - INFO - Processing year 1985\n",
      "2025-03-23 09:55:13,748 - INFO - Processing year 1986\n",
      "2025-03-23 09:55:15,247 - INFO - Processing year 1987\n",
      "2025-03-23 09:55:16,498 - INFO - Processing year 1988\n",
      "2025-03-23 09:55:17,945 - INFO - Processing year 1989\n",
      "2025-03-23 09:55:19,382 - INFO - Processing year 1990\n",
      "2025-03-23 09:55:21,120 - INFO - Processing year 1991\n",
      "2025-03-23 09:55:22,391 - INFO - Processing year 1992\n",
      "2025-03-23 09:55:23,814 - INFO - Processing year 1993\n",
      "2025-03-23 09:55:25,527 - INFO - Processing year 1994\n",
      "2025-03-23 09:55:26,553 - INFO - Time elapsed: 21.02 seconds\n",
      "2025-03-23 09:55:26,554 - INFO - Memory usage: 0.85 MB\n",
      "2025-03-23 09:55:26,556 - INFO - CPU usage: 66.15%\n",
      "2025-03-23 09:55:26,557 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:55:27,618 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:55:27,640 - INFO - Processing year 1995\n",
      "2025-03-23 09:55:28,684 - INFO - Processing year 1996\n",
      "2025-03-23 09:55:30,143 - INFO - Processing year 1997\n",
      "2025-03-23 09:55:31,740 - INFO - Processing year 1998\n",
      "2025-03-23 09:55:32,763 - INFO - Processing year 1999\n",
      "2025-03-23 09:55:34,212 - INFO - Processing year 2000\n",
      "2025-03-23 09:55:35,635 - INFO - Processing year 2001\n",
      "2025-03-23 09:55:37,089 - INFO - Processing year 2002\n",
      "2025-03-23 09:55:38,089 - INFO - Processing year 2003\n",
      "2025-03-23 09:55:39,745 - INFO - Processing year 2004\n",
      "2025-03-23 09:55:41,458 - INFO - Processing year 2005\n",
      "2025-03-23 09:55:43,194 - INFO - Processing year 2006\n",
      "2025-03-23 09:55:44,218 - INFO - Processing year 2007\n",
      "2025-03-23 09:55:45,862 - INFO - Processing year 2008\n",
      "2025-03-23 09:55:47,539 - INFO - Processing year 2009\n",
      "2025-03-23 09:55:48,542 - INFO - Processing year 2010\n",
      "2025-03-23 09:55:50,124 - INFO - Time elapsed: 22.50 seconds\n",
      "2025-03-23 09:55:50,126 - INFO - Memory usage: 2.86 MB\n",
      "2025-03-23 09:55:50,128 - INFO - CPU usage: 48.95%\n",
      "2025-03-23 09:55:50,129 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:55:51,297 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:55:51,301 - INFO - Completed processing: time=45.99s, CPU=3.33%, memory=3.70MB\n",
      "2025-03-23 09:55:56,313 - INFO - Running scalability analysis for dataset_size=medium, cluster_size=8\n",
      "2025-03-23 09:55:56,317 - INFO - Starting processing for dataset_size=medium, cluster_size=8\n",
      "2025-03-23 09:55:56,512 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:55:56,534 - INFO - Processing year 1980\n",
      "2025-03-23 09:55:57,567 - INFO - Processing year 1981\n",
      "2025-03-23 09:55:58,980 - INFO - Processing year 1982\n",
      "2025-03-23 09:56:00,432 - INFO - Processing year 1983\n",
      "2025-03-23 09:56:01,442 - INFO - Processing year 1984\n",
      "2025-03-23 09:56:02,867 - INFO - Processing year 1985\n",
      "2025-03-23 09:56:04,552 - INFO - Processing year 1986\n",
      "2025-03-23 09:56:06,008 - INFO - Processing year 1987\n",
      "2025-03-23 09:56:07,140 - INFO - Processing year 1988\n",
      "2025-03-23 09:56:08,638 - INFO - Processing year 1989\n",
      "2025-03-23 09:56:10,119 - INFO - Processing year 1990\n",
      "2025-03-23 09:56:11,557 - INFO - Processing year 1991\n",
      "2025-03-23 09:56:12,598 - INFO - Processing year 1992\n",
      "2025-03-23 09:56:14,142 - INFO - Processing year 1993\n",
      "2025-03-23 09:56:15,808 - INFO - Processing year 1994\n",
      "2025-03-23 09:56:16,856 - INFO - Time elapsed: 20.34 seconds\n",
      "2025-03-23 09:56:16,858 - INFO - Memory usage: 0.05 MB\n",
      "2025-03-23 09:56:16,859 - INFO - CPU usage: 16.35%\n",
      "2025-03-23 09:56:16,860 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:56:17,925 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:56:17,949 - INFO - Processing year 1995\n",
      "2025-03-23 09:56:19,193 - INFO - Processing year 1996\n",
      "2025-03-23 09:56:20,579 - INFO - Processing year 1997\n",
      "2025-03-23 09:56:22,061 - INFO - Processing year 1998\n",
      "2025-03-23 09:56:23,108 - INFO - Processing year 1999\n",
      "2025-03-23 09:56:24,765 - INFO - Processing year 2000\n",
      "2025-03-23 09:56:26,368 - INFO - Processing year 2001\n",
      "2025-03-23 09:56:27,835 - INFO - Processing year 2002\n",
      "2025-03-23 09:56:29,079 - INFO - Processing year 2003\n",
      "2025-03-23 09:56:30,471 - INFO - Processing year 2004\n",
      "2025-03-23 09:56:32,028 - INFO - Processing year 2005\n",
      "2025-03-23 09:56:33,472 - INFO - Processing year 2006\n",
      "2025-03-23 09:56:34,485 - INFO - Processing year 2007\n",
      "2025-03-23 09:56:35,911 - INFO - Processing year 2008\n",
      "2025-03-23 09:56:37,633 - INFO - Processing year 2009\n",
      "2025-03-23 09:56:38,670 - INFO - Processing year 2010\n",
      "2025-03-23 09:56:40,003 - INFO - Time elapsed: 22.07 seconds\n",
      "2025-03-23 09:56:40,006 - INFO - Memory usage: -0.03 MB\n",
      "2025-03-23 09:56:40,008 - INFO - CPU usage: 40.65%\n",
      "2025-03-23 09:56:40,010 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:56:41,231 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:56:41,235 - INFO - Completed processing: time=44.92s, CPU=3.40%, memory=0.02MB\n",
      "2025-03-23 09:56:46,246 - INFO - Running scalability analysis for dataset_size=large, cluster_size=2\n",
      "2025-03-23 09:56:46,249 - INFO - Starting processing for dataset_size=large, cluster_size=2\n",
      "2025-03-23 09:56:46,458 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:56:46,476 - INFO - Processing year 1980\n",
      "2025-03-23 09:56:47,720 - INFO - Processing year 1981\n",
      "2025-03-23 09:56:49,435 - INFO - Processing year 1982\n",
      "2025-03-23 09:56:50,907 - INFO - Processing year 1983\n",
      "2025-03-23 09:56:51,963 - INFO - Processing year 1984\n",
      "2025-03-23 09:56:53,632 - INFO - Processing year 1985\n",
      "2025-03-23 09:56:55,066 - INFO - Processing year 1986\n",
      "2025-03-23 09:56:56,812 - INFO - Processing year 1987\n",
      "2025-03-23 09:56:57,842 - INFO - Processing year 1988\n",
      "2025-03-23 09:56:59,260 - INFO - Processing year 1989\n",
      "2025-03-23 09:57:00,709 - INFO - Processing year 1990\n",
      "2025-03-23 09:57:02,146 - INFO - Processing year 1991\n",
      "2025-03-23 09:57:03,167 - INFO - Processing year 1992\n",
      "2025-03-23 09:57:04,846 - INFO - Processing year 1993\n",
      "2025-03-23 09:57:06,587 - INFO - Processing year 1994\n",
      "2025-03-23 09:57:07,849 - INFO - Time elapsed: 21.39 seconds\n",
      "2025-03-23 09:57:07,852 - INFO - Memory usage: 0.05 MB\n",
      "2025-03-23 09:57:07,855 - INFO - CPU usage: 16.50%\n",
      "2025-03-23 09:57:07,856 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:57:08,945 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:57:08,970 - INFO - Processing year 1995\n",
      "2025-03-23 09:57:10,222 - INFO - Processing year 1996\n",
      "2025-03-23 09:57:11,929 - INFO - Processing year 1997\n",
      "2025-03-23 09:57:13,502 - INFO - Processing year 1998\n",
      "2025-03-23 09:57:14,530 - INFO - Processing year 1999\n",
      "2025-03-23 09:57:15,921 - INFO - Processing year 2000\n",
      "2025-03-23 09:57:17,607 - INFO - Processing year 2001\n",
      "2025-03-23 09:57:19,309 - INFO - Processing year 2002\n",
      "2025-03-23 09:57:20,574 - INFO - Processing year 2003\n",
      "2025-03-23 09:57:21,991 - INFO - Processing year 2004\n",
      "2025-03-23 09:57:23,710 - INFO - Processing year 2005\n",
      "2025-03-23 09:57:25,171 - INFO - Processing year 2006\n",
      "2025-03-23 09:57:26,179 - INFO - Processing year 2007\n",
      "2025-03-23 09:57:27,615 - INFO - Processing year 2008\n",
      "2025-03-23 09:57:29,063 - INFO - Processing year 2009\n",
      "2025-03-23 09:57:30,158 - INFO - Processing year 2010\n",
      "2025-03-23 09:57:31,443 - INFO - Time elapsed: 22.49 seconds\n",
      "2025-03-23 09:57:31,446 - INFO - Memory usage: 0.03 MB\n",
      "2025-03-23 09:57:31,449 - INFO - CPU usage: 32.35%\n",
      "2025-03-23 09:57:31,452 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:57:32,525 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-23 09:57:32,652 - INFO - Processing year 2011\n",
      "2025-03-23 09:57:34,062 - INFO - Processing year 2012\n",
      "2025-03-23 09:57:35,746 - INFO - Processing year 2013\n",
      "2025-03-23 09:57:37,451 - INFO - Processing year 2014\n",
      "2025-03-23 09:57:38,476 - INFO - Processing year 2015\n",
      "2025-03-23 09:57:40,258 - INFO - Processing year 2016\n",
      "2025-03-23 09:57:42,117 - INFO - Processing year 2017\n",
      "2025-03-23 09:57:43,966 - INFO - Processing year 2018\n",
      "2025-03-23 09:57:45,071 - INFO - Processing year 2019\n",
      "2025-03-23 09:57:46,920 - INFO - Processing year 2020\n",
      "2025-03-23 09:57:48,666 - INFO - Processing year 2021\n",
      "2025-03-23 09:57:50,342 - INFO - Processing year 2022\n",
      "2025-03-23 09:57:51,575 - INFO - Processing year 2023\n",
      "2025-03-23 09:57:53,890 - INFO - Processing year 2024\n",
      "2025-03-23 09:57:54,823 - INFO - Time elapsed: 22.29 seconds\n",
      "2025-03-23 09:57:54,826 - INFO - Memory usage: 0.05 MB\n",
      "2025-03-23 09:57:54,828 - INFO - CPU usage: 30.20%\n",
      "2025-03-23 09:57:54,830 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:57:56,322 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:57:56,325 - INFO - Completed processing: time=70.07s, CPU=3.40%, memory=0.13MB\n",
      "2025-03-23 09:58:01,337 - INFO - Running scalability analysis for dataset_size=large, cluster_size=4\n",
      "2025-03-23 09:58:01,339 - INFO - Starting processing for dataset_size=large, cluster_size=4\n",
      "2025-03-23 09:58:01,551 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:58:01,570 - INFO - Processing year 1980\n",
      "2025-03-23 09:58:02,578 - INFO - Processing year 1981\n",
      "2025-03-23 09:58:04,049 - INFO - Processing year 1982\n",
      "2025-03-23 09:58:05,622 - INFO - Processing year 1983\n",
      "2025-03-23 09:58:06,642 - INFO - Processing year 1984\n",
      "2025-03-23 09:58:08,309 - INFO - Processing year 1985\n",
      "2025-03-23 09:58:09,981 - INFO - Processing year 1986\n",
      "2025-03-23 09:58:11,422 - INFO - Processing year 1987\n",
      "2025-03-23 09:58:12,438 - INFO - Processing year 1988\n",
      "2025-03-23 09:58:13,857 - INFO - Processing year 1989\n",
      "2025-03-23 09:58:15,332 - INFO - Processing year 1990\n",
      "2025-03-23 09:58:16,799 - INFO - Processing year 1991\n",
      "2025-03-23 09:58:17,830 - INFO - Processing year 1992\n",
      "2025-03-23 09:58:19,518 - INFO - Processing year 1993\n",
      "2025-03-23 09:58:21,060 - INFO - Processing year 1994\n",
      "2025-03-23 09:58:22,078 - INFO - Time elapsed: 20.52 seconds\n",
      "2025-03-23 09:58:22,080 - INFO - Memory usage: 0.01 MB\n",
      "2025-03-23 09:58:22,081 - INFO - CPU usage: 15.90%\n",
      "2025-03-23 09:58:22,082 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:58:23,149 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:58:23,176 - INFO - Processing year 1995\n",
      "2025-03-23 09:58:24,181 - INFO - Processing year 1996\n",
      "2025-03-23 09:58:25,594 - INFO - Processing year 1997\n",
      "2025-03-23 09:58:27,301 - INFO - Processing year 1998\n",
      "2025-03-23 09:58:28,347 - INFO - Processing year 1999\n",
      "2025-03-23 09:58:29,757 - INFO - Processing year 2000\n",
      "2025-03-23 09:58:31,364 - INFO - Processing year 2001\n",
      "2025-03-23 09:58:32,795 - INFO - Processing year 2002\n",
      "2025-03-23 09:58:33,802 - INFO - Processing year 2003\n",
      "2025-03-23 09:58:35,192 - INFO - Processing year 2004\n",
      "2025-03-23 09:58:36,653 - INFO - Processing year 2005\n",
      "2025-03-23 09:58:38,208 - INFO - Processing year 2006\n",
      "2025-03-23 09:58:39,205 - INFO - Processing year 2007\n",
      "2025-03-23 09:58:40,770 - INFO - Processing year 2008\n",
      "2025-03-23 09:58:42,463 - INFO - Processing year 2009\n",
      "2025-03-23 09:58:43,472 - INFO - Processing year 2010\n",
      "2025-03-23 09:58:45,056 - INFO - Time elapsed: 21.90 seconds\n",
      "2025-03-23 09:58:45,058 - INFO - Memory usage: 2.87 MB\n",
      "2025-03-23 09:58:45,060 - INFO - CPU usage: 15.50%\n",
      "2025-03-23 09:58:45,061 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:58:46,125 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-23 09:58:46,149 - INFO - Processing year 2011\n",
      "2025-03-23 09:58:47,161 - INFO - Processing year 2012\n",
      "2025-03-23 09:58:48,663 - INFO - Processing year 2013\n",
      "2025-03-23 09:58:50,139 - INFO - Processing year 2014\n",
      "2025-03-23 09:58:51,178 - INFO - Processing year 2015\n",
      "2025-03-23 09:58:52,598 - INFO - Processing year 2016\n",
      "2025-03-23 09:58:54,296 - INFO - Processing year 2017\n",
      "2025-03-23 09:58:55,766 - INFO - Processing year 2018\n",
      "2025-03-23 09:58:56,770 - INFO - Processing year 2019\n",
      "2025-03-23 09:58:58,417 - INFO - Processing year 2020\n",
      "2025-03-23 09:59:00,093 - INFO - Processing year 2021\n",
      "2025-03-23 09:59:01,579 - INFO - Processing year 2022\n",
      "2025-03-23 09:59:02,591 - INFO - Processing year 2023\n",
      "2025-03-23 09:59:03,973 - INFO - Processing year 2024\n",
      "2025-03-23 09:59:04,939 - INFO - Time elapsed: 18.81 seconds\n",
      "2025-03-23 09:59:04,941 - INFO - Memory usage: 0.05 MB\n",
      "2025-03-23 09:59:04,942 - INFO - CPU usage: 40.35%\n",
      "2025-03-23 09:59:04,943 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:59:06,324 - INFO - Spark session stopped successfully\n",
      "2025-03-23 09:59:06,327 - INFO - Completed processing: time=64.98s, CPU=3.55%, memory=2.93MB\n",
      "2025-03-23 09:59:11,338 - INFO - Running scalability analysis for dataset_size=large, cluster_size=8\n",
      "2025-03-23 09:59:11,341 - INFO - Starting processing for dataset_size=large, cluster_size=8\n",
      "2025-03-23 09:59:11,508 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\n",
      "2025-03-23 09:59:11,527 - INFO - Processing year 1980\n",
      "2025-03-23 09:59:12,531 - INFO - Processing year 1981\n",
      "2025-03-23 09:59:13,998 - INFO - Processing year 1982\n",
      "2025-03-23 09:59:15,635 - INFO - Processing year 1983\n",
      "2025-03-23 09:59:16,639 - INFO - Processing year 1984\n",
      "2025-03-23 09:59:18,079 - INFO - Processing year 1985\n",
      "2025-03-23 09:59:19,531 - INFO - Processing year 1986\n",
      "2025-03-23 09:59:21,000 - INFO - Processing year 1987\n",
      "2025-03-23 09:59:22,011 - INFO - Processing year 1988\n",
      "2025-03-23 09:59:23,402 - INFO - Processing year 1989\n",
      "2025-03-23 09:59:24,863 - INFO - Processing year 1990\n",
      "2025-03-23 09:59:26,573 - INFO - Processing year 1991\n",
      "2025-03-23 09:59:27,600 - INFO - Processing year 1992\n",
      "2025-03-23 09:59:29,055 - INFO - Processing year 1993\n",
      "2025-03-23 09:59:30,515 - INFO - Processing year 1994\n",
      "2025-03-23 09:59:31,544 - INFO - Time elapsed: 20.03 seconds\n",
      "2025-03-23 09:59:31,546 - INFO - Memory usage: 0.02 MB\n",
      "2025-03-23 09:59:31,547 - INFO - CPU usage: 16.20%\n",
      "2025-03-23 09:59:31,548 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:59:32,610 - INFO - Processing file: tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\n",
      "2025-03-23 09:59:32,632 - INFO - Processing year 1995\n",
      "2025-03-23 09:59:33,668 - INFO - Processing year 1996\n",
      "2025-03-23 09:59:35,099 - INFO - Processing year 1997\n",
      "2025-03-23 09:59:36,569 - INFO - Processing year 1998\n",
      "2025-03-23 09:59:37,582 - INFO - Processing year 1999\n",
      "2025-03-23 09:59:39,241 - INFO - Processing year 2000\n",
      "2025-03-23 09:59:40,708 - INFO - Processing year 2001\n",
      "2025-03-23 09:59:42,197 - INFO - Processing year 2002\n",
      "2025-03-23 09:59:43,269 - INFO - Processing year 2003\n",
      "2025-03-23 09:59:44,851 - INFO - Processing year 2004\n",
      "2025-03-23 09:59:46,553 - INFO - Processing year 2005\n",
      "2025-03-23 09:59:47,991 - INFO - Processing year 2006\n",
      "2025-03-23 09:59:49,005 - INFO - Processing year 2007\n",
      "2025-03-23 09:59:50,697 - INFO - Processing year 2008\n",
      "2025-03-23 09:59:52,409 - INFO - Processing year 2009\n",
      "2025-03-23 09:59:53,438 - INFO - Processing year 2010\n",
      "2025-03-23 09:59:54,734 - INFO - Time elapsed: 22.12 seconds\n",
      "2025-03-23 09:59:54,735 - INFO - Memory usage: -0.03 MB\n",
      "2025-03-23 09:59:54,736 - INFO - CPU usage: 15.45%\n",
      "2025-03-23 09:59:54,737 - ERROR - Error processing tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 09:59:55,800 - INFO - Processing file: tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\n",
      "2025-03-23 09:59:55,825 - INFO - Processing year 2011\n",
      "2025-03-23 09:59:56,854 - INFO - Processing year 2012\n",
      "2025-03-23 09:59:58,385 - INFO - Processing year 2013\n",
      "2025-03-23 09:59:59,838 - INFO - Processing year 2014\n",
      "2025-03-23 10:00:00,841 - INFO - Processing year 2015\n",
      "2025-03-23 10:00:02,338 - INFO - Processing year 2016\n",
      "2025-03-23 10:00:03,764 - INFO - Processing year 2017\n",
      "2025-03-23 10:00:05,251 - INFO - Processing year 2018\n",
      "2025-03-23 10:00:06,264 - INFO - Processing year 2019\n",
      "2025-03-23 10:00:07,907 - INFO - Processing year 2020\n",
      "2025-03-23 10:00:09,573 - INFO - Processing year 2021\n",
      "2025-03-23 10:00:11,249 - INFO - Processing year 2022\n",
      "2025-03-23 10:00:12,328 - INFO - Processing year 2023\n",
      "2025-03-23 10:00:13,956 - INFO - Processing year 2024\n",
      "2025-03-23 10:00:14,817 - INFO - Time elapsed: 19.01 seconds\n",
      "2025-03-23 10:00:14,820 - INFO - Memory usage: 0.05 MB\n",
      "2025-03-23 10:00:14,821 - INFO - CPU usage: 15.35%\n",
      "2025-03-23 10:00:14,822 - ERROR - Error processing tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `year`.\n",
      "2025-03-23 10:00:15,968 - INFO - Spark session stopped successfully\n",
      "2025-03-23 10:00:15,972 - INFO - Completed processing: time=64.63s, CPU=3.75%, memory=0.04MB\n",
      "2025-03-23 10:00:21,161 - INFO - Scalability analysis complete. Results saved to scalability_results.csv\n",
      "2025-03-23 10:00:21,163 - INFO - Summary saved to scalability_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, avg\n",
    "import xarray as xr\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"scalability_analysis.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Configuration for scalability analysis\n",
    "DATASET_SIZES = [\"small\", \"medium\", \"large\"]  # Define dataset sizes\n",
    "CLUSTER_SIZES = [2, 4, 8]  # Define cluster sizes \n",
    "OUTPUT_DIR = \"scalability_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "@contextmanager\n",
    "def memory_monitoring():\n",
    "    \"\"\"Context manager to monitor memory usage during operations.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    cpu_before = psutil.cpu_percent()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        mem_after = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "        cpu_after = psutil.cpu_percent()\n",
    "        \n",
    "        logger.info(f\"Time elapsed: {end_time - start_time:.2f} seconds\")\n",
    "        logger.info(f\"Memory usage: {mem_after - mem_before:.2f} MB\")\n",
    "        logger.info(f\"CPU usage: {(cpu_before + cpu_after) / 2:.2f}%\")\n",
    "\n",
    "def get_spark_session(cluster_size):\n",
    "    \"\"\"Create and configure a Spark session with appropriate resources.\"\"\"\n",
    "    try:\n",
    "        # Configure Spark with memory constraints and cluster size\n",
    "        return SparkSession.builder\\\n",
    "            .master(\"spark://192.168.2.156:7077\") \\\n",
    "            .appName(\"scalability_group_28\")\\\n",
    "            .config(\"spark.executor.memory\", \"2g\")\\\n",
    "            .config(\"spark.driver.memory\", \"4g\")\\\n",
    "            .config(\"spark.memory.offHeap.enabled\", \"true\")\\\n",
    "            .config(\"spark.memory.offHeap.size\", \"2g\")\\\n",
    "            .config(\"spark.executor.cores\", str(cluster_size))\\\n",
    "            .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "            .getOrCreate()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Spark: {e}\")\n",
    "        raise\n",
    "\n",
    "def process_netcdf_chunk(file_path, spark, chunk_size=500):\n",
    "    \"\"\"Process a NetCDF file in chunks to avoid memory issues.\"\"\"\n",
    "    logger.info(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with memory_monitoring():\n",
    "            # Open the NetCDF file with chunking options\n",
    "            with xr.open_dataset(file_path, chunks={'time': chunk_size}) as ds:\n",
    "                # Instead of loading the entire dataset at once, we'll process it in chunks\n",
    "                yearly_avgs = []\n",
    "                \n",
    "                # Get unique years\n",
    "                time_values = ds.time.values\n",
    "                years = np.unique(pd.DatetimeIndex(time_values).year)\n",
    "                \n",
    "                # Process each year separately\n",
    "                for yr in years:\n",
    "                    logger.info(f\"Processing year {yr}\")\n",
    "                    # Filter dataset for current year\n",
    "                    year_ds = ds.sel(time=ds.time.dt.year == yr)\n",
    "                    \n",
    "                    # Extract necessary variables \n",
    "                    if 'tg' in year_ds:\n",
    "                        # Calculate the mean for this year\n",
    "                        yearly_avg = year_ds.tg.mean().compute()\n",
    "                        yearly_avgs.append((yr, float(yearly_avg)))\n",
    "                    \n",
    "                    # garbage collection\n",
    "                    del year_ds\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Create spark dataframe \n",
    "                if yearly_avgs:\n",
    "                    schema = [\"year\", \"yearly_avg_temperature\"]\n",
    "                    spark_df = spark.createDataFrame(yearly_avgs, schema=schema)\n",
    "                    # Cache small result\n",
    "                    spark_df.cache()\n",
    "                    # Collect results \n",
    "                    results = spark_df.collect()\n",
    "                    spark_df.unpersist()\n",
    "                    return results\n",
    "                return []\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def run_netcdf_processing(dataset_size, cluster_size):\n",
    "    \"\"\"\n",
    "    Run the NetCDF processing script with the given dataset size and cluster size.\n",
    "    \"\"\"\n",
    "    # Define file paths based on dataset size\n",
    "    if dataset_size == \"small\":\n",
    "        file_paths = [\"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\"]\n",
    "    elif dataset_size == \"medium\":\n",
    "        file_paths = [\n",
    "            \"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\"\n",
    "        ]\n",
    "    elif dataset_size == \"large\":\n",
    "        file_paths = [\n",
    "            \"tg_ens_mean_0.25deg_reg_1980-1994_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_1995-2010_v30.0e.nc\",\n",
    "            \"tg_ens_mean_0.25deg_reg_2011-2024_v30.0e.nc\"\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset size\")\n",
    "    \n",
    "    logger.info(f\"Starting processing for dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "    \n",
    "    # Initialize metrics\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_usage_samples = []\n",
    "    memory_usage_start = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "    spark = None\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Start Spark session with the given cluster size\n",
    "        spark = get_spark_session(cluster_size)\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in file_paths:\n",
    "            # Sample CPU usage periodically\n",
    "            cpu_usage_samples.append(psutil.cpu_percent())\n",
    "            \n",
    "            # Process the file\n",
    "            file_results = process_netcdf_chunk(file_path, spark)\n",
    "            results.extend(file_results)\n",
    "            \n",
    "            # Garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "            # Let system resources recover between files\n",
    "            time.sleep(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in processing run: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Stop the Spark session\n",
    "        if spark:\n",
    "            try:\n",
    "                spark.stop()\n",
    "                logger.info(\"Spark session stopped successfully\")\n",
    "            except:\n",
    "                logger.warning(\"Error stopping Spark session\")\n",
    "    \n",
    "    # Log end time and resource usage\n",
    "    end_time = time.time()\n",
    "    cpu_usage_samples.append(psutil.cpu_percent())\n",
    "    memory_usage_end = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Calculate metrics\n",
    "    processing_time = end_time - start_time\n",
    "    cpu_usage = sum(cpu_usage_samples) / len(cpu_usage_samples) if cpu_usage_samples else 0\n",
    "    memory_usage = memory_usage_end - memory_usage_start\n",
    "    \n",
    "    logger.info(f\"Completed processing: time={processing_time:.2f}s, CPU={cpu_usage:.2f}%, memory={memory_usage:.2f}MB\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"cluster_size\": cluster_size,\n",
    "        \"processing_time\": processing_time,\n",
    "        \"cpu_usage\": cpu_usage,\n",
    "        \"memory_usage\": memory_usage,\n",
    "        \"result_count\": len(results)\n",
    "    }\n",
    "\n",
    "def perform_scalability_analysis():\n",
    "    \"\"\"\n",
    "    Perform scalability analysis by running the NetCDF processing script with different configurations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create a checkpoint file \n",
    "    checkpoint_file = os.path.join(OUTPUT_DIR, \"checkpoint.csv\")\n",
    "    completed_configs = set()\n",
    "    \n",
    "    # Check if checkpoint exists and load completed configurations\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint_df = pd.read_csv(checkpoint_file)\n",
    "        for _, row in checkpoint_df.iterrows():\n",
    "            completed_configs.add((row['dataset_size'], row['cluster_size']))\n",
    "        logger.info(f\"Loaded {len(completed_configs)} completed configurations from checkpoint\")\n",
    "        results = checkpoint_df.to_dict('records')\n",
    "    \n",
    "    # Run the analysis for each configuration\n",
    "    for dataset_size in DATASET_SIZES:\n",
    "        for cluster_size in CLUSTER_SIZES:\n",
    "            # Skip configurations that have already been completed\n",
    "            if (dataset_size, cluster_size) in completed_configs:\n",
    "                logger.info(f\"Skipping completed configuration: dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Running scalability analysis for dataset_size={dataset_size}, cluster_size={cluster_size}\")\n",
    "            \n",
    "            try:\n",
    "                # Run the processing with this configuration\n",
    "                result = run_netcdf_processing(dataset_size, cluster_size)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update checkpoint file after each successful configuration\n",
    "                pd.DataFrame(results).to_csv(checkpoint_file, index=False)\n",
    "                completed_configs.add((dataset_size, cluster_size))\n",
    "                \n",
    "                # Stabilize the system \n",
    "                time.sleep(5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed analysis for dataset_size={dataset_size}, cluster_size={cluster_size}: {e}\")\n",
    "    \n",
    "    # Save final results to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(os.path.join(OUTPUT_DIR, \"scalability_results.csv\"), index=False)\n",
    "    \n",
    "    # Generate a simple summary\n",
    "    summary = results_df.groupby(['dataset_size', 'cluster_size']).agg({\n",
    "        'processing_time': 'mean',\n",
    "        'cpu_usage': 'mean',\n",
    "        'memory_usage': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    summary.to_csv(os.path.join(OUTPUT_DIR, \"scalability_summary.csv\"), index=False)\n",
    "    \n",
    "    logger.info(\"Scalability analysis complete. Results saved to scalability_results.csv\")\n",
    "    logger.info(\"Summary saved to scalability_summary.csv\")\n",
    "\n",
    "# Run scalability analysis\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        perform_scalability_analysis()\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unhandled exception in main: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34b92f-d685-479b-9fe7-589a48743ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
